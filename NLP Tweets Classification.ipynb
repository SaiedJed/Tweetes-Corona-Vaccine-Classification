{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7efe140",
   "metadata": {},
   "source": [
    "# Step [1]: Prepare libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b1951",
   "metadata": {},
   "source": [
    "## [1.1] Include important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6058ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229df102",
   "metadata": {},
   "source": [
    "## [1.2] Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5504e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'https://drive.google.com/uc?export=download: Scheme missing.\n",
      "'id' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1KepfzAhJ7dloG8XaWQf0ovQipDHYS8aI' -O 'final_data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530e1bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find either final_data.zip or final_data.zip.zip.\n"
     ]
    }
   ],
   "source": [
    "!unzip final_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400638b",
   "metadata": {},
   "source": [
    "## [1.3] read data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41a9612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8117\n",
      "2717\n",
      "2688\n"
     ]
    }
   ],
   "source": [
    "tr = pd.read_csv(\"train.csv\")\n",
    "te = pd.read_csv(\"test.csv\")\n",
    "va = pd.read_csv(\"valid.csv\")\n",
    "    \n",
    "train = tr[(tr['label'] == 0)|(tr['label'] == 1)]\n",
    "print(len(train))\n",
    "test = te[(te['label'] == 0)|(te['label'] == 1)]\n",
    "print(len(test))\n",
    "valid = va[(va['label'] == 0)|(va['label'] == 1)]\n",
    "print(len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec48c7b",
   "metadata": {},
   "source": [
    "## [1.4] Prapere The Comparison Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfcdcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0428b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'] = []\n",
    "model_comparison_table['preprocessing_methods'] = []\n",
    "model_comparison_table['accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22713d",
   "metadata": {},
   "source": [
    "# Step [2]: Build Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef90d1",
   "metadata": {},
   "source": [
    "## [2.1] Extract Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b8c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = train['tweet'].tolist()\n",
    "X_train,vec = getV(trainx)\n",
    "Y_train = train['label'].tolist()\n",
    "\n",
    "X_test = test['tweet'].tolist()\n",
    "X_test = vec.transform(X_test)\n",
    "Y_test = test['label'].tolist()\n",
    "\n",
    "X_valid = valid['tweet'].tolist()\n",
    "X_valid = vec.transform(X_valid)\n",
    "Y_valid = valid['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9a20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getV(d):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_d = vectorizer.fit_transform(d) \n",
    "    return X_d,vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24164796",
   "metadata": {},
   "source": [
    "## [2.2] Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e373c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_predicted_train = clf.predict(X_train)\n",
    "y_predicted_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286e05a",
   "metadata": {},
   "source": [
    "## [2.3] Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e011a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9949488727362326\n",
      "Test : 0.7765918292234082\n"
     ]
    }
   ],
   "source": [
    "print(\"Train : \" + str(accuracy_score(Y_train, y_predicted_train)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_test)\n",
    "print(\"Test : \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b0c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"none\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e52feb",
   "metadata": {},
   "source": [
    "# Step [3]: Build model with preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07701466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9926081064432672\n",
      "Test : 0.7769598822230401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[1,0,0,0,0,0,0,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[1,0,0,0,0,0,0,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing URLs\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ce86ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9932240975729949\n",
      "Test : 0.771071034228929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,1,0,0,0,0,0,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,1,0,0,0,0,0,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Mentions\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74432f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.994579278058396\n",
      "Test : 0.7799043062200957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,1,0,0,0,0,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,1,0,0,0,0,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Repeated\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b658d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9950720709621781\n",
      "Test : 0.7743835112256164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,1,0,0,0,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,1,0,0,0,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"Unify words\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea5c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9947024762843415\n",
      "Test : 0.770702981229297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,1,0,0,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,1,0,0,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"Unify characters\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f45c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.994579278058396\n",
      "Test : 0.7810084652189916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,0,1,0,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,0,1,0,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Numbers\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661e42b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9949488727362326\n",
      "Test : 0.7754876702245124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,0,0,1,0,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,0,0,1,0,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Emojis\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c060c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9942096833805594\n",
      "Test : 0.7747515642252485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,0,0,0,1,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,0,0,0,1,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Stop-words\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5403200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9743747690033263\n",
      "Test : 0.7703349282296651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,0,0,0,0,1,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,0,0,0,0,1,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"Stemming\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0583a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9876801774054453\n",
      "Test : 0.7732793522267206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,0,0,0,0,0,1,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,0,0,0,0,0,1,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Non-arabic\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b9e3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9949488727362326\n",
      "Test : 0.7765918292234082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[0,0,0,0,0,0,0,0,0,0,1])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[0,0,0,0,0,0,0,0,0,0,1])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)\n",
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing Punctuaion\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74691713",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train.copy(deep=True)\n",
    "preprocessed = preprocess(temp,'tweet',[1,0,0,1,0,1,0,1,0,0,0])\n",
    "pre = preprocessed['tweet'].tolist()\n",
    "X_pre,vec = getV(pre)\n",
    "Y_pre = preprocessed['label'].tolist()\n",
    "\n",
    "tempt = test.copy(deep=True)\n",
    "tempt = preprocess(tempt,'tweet',[1,0,0,1,0,1,0,1,0,0,0])\n",
    "X_test = tempt['tweet'].tolist()\n",
    "X_test_pre = vec.transform(X_test)\n",
    "Y_test = tempt['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b552794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheSo\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_pre, Y_pre)\n",
    "y_predicted_trainp = clf.predict(X_pre)\n",
    "y_predicted_testp = clf.predict(X_test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bbe18ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.9903905383762474\n",
      "Test : 0.7824806772175193\n"
     ]
    }
   ],
   "source": [
    "print(\"Train : \" + str(accuracy_score(Y_pre, y_predicted_trainp)))\n",
    "accuracy = accuracy_score(Y_test, y_predicted_testp)\n",
    "print(\"Test : \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b099f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing URLs, unifying Words, removing Numbers, removing Stop-words\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d605c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(tweet, work=1):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "        work: binary value take 1 by default working as on/off for the function, if work=0 the function will return\n",
    "     the tweet without changing\n",
    "    Output:\n",
    "        cleaned_tweet: tweet after removing urls\n",
    "\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    \n",
    "    tweet = re.sub(r'https?\\S+', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_mentions(tweet,work=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة المنظفة\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    tweet = re.sub(r\"@\\S+\",'', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_repeated(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة المنظفة\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    \n",
    "    tweet = re.compile(r'(.)\\1{1,}', re.IGNORECASE).sub(r'\\1', tweet)\n",
    "        \n",
    "    return tweet\n",
    "\n",
    "def unify(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مستبدلة المرادفات بالكلمة المحددة لها\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    synonyms = {\"تلقيح\":['تطعيم']\n",
    "        ,\"كورونا\":['كوفيد-19','كرونا']}\n",
    "    for main,syn in synonyms.items():\n",
    "        for s in syn:\n",
    "            tweet = re.sub(str(s),str(main),tweet)\n",
    "    return tweet   \n",
    "\n",
    "def remove_numbers(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مزالة الارقام منها\n",
    "    \"\"\"\n",
    "     \n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    tweet = re.sub(r\"[1-9]\",\"\",tweet)\n",
    "    tweet = re.sub(r\"[\\u0660-\\u0669]+\",\"\",tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_punc(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مزالة علامات الترقيم منها\n",
    "    \"\"\"\n",
    "     \n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    tok = RegexpTokenizer(r'\\w+')\n",
    "    tweet = ' '.join(tok.tokenize(tweet))\n",
    "    return tweet\n",
    "    \n",
    "def remove_emojis(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة المنظفة\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"  u\"\\u2600-\\u2B55\" u\"\\u200d\" u\"\\u23cf\" u\"\\u23e9\" u\"\\u231a\"u\"\\ufe0f\"  u\"\\u3030\"  \"]+\", re.UNICODE)\n",
    "    tweet = re.sub(pattern,'',tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def remove_sWords(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مزالة كلمات التوقف منها\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    temp = []\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    tweet = tweet.split()\n",
    "    for s in tweet:\n",
    "        if s not in stop_words:\n",
    "            temp.append(s)\n",
    "    tweet = \" \".join(temp)\n",
    "    return tweet\n",
    "\n",
    "def remove_nonar(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مزالة الأحرف غير العربية منها\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    \n",
    "    tweet =\" \".join(re.findall(r'[(#)?\\u0600-\\u06FF]+', tweet))\n",
    "    return tweet\n",
    "\n",
    "def stemming(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مستبدلة الكلمات بجذرها \n",
    "    \"\"\"\n",
    "    if work == 0 :\n",
    "        return tweet\n",
    "    temp = []\n",
    "    #tokenizer = TweetTokenizer()\n",
    "    #tokens = tokenizer.tokenize(tweet)\n",
    "    tokens = tweet.split(\" \")\n",
    "    porter = ISRIStemmer()\n",
    "    for t in tokens:\n",
    "        if '#' not in t:\n",
    "            temp.append(porter.stem(t))\n",
    "        else:\n",
    "            temp.append(t)             \n",
    "        \n",
    "    return \" \".join(temp)\n",
    "\n",
    "def unify_c(tweet,work = 1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "     لطلب تنفيذها flagالتغريدة وال\n",
    "    Output:\n",
    "    التغريدة مستبدلة الاحرف بالاحرف الموحدة\n",
    "    \"\"\"\n",
    "    if work == 0:\n",
    "        return tweet\n",
    "    synonyms = {\"ى\":['ي']\n",
    "        ,\"ا\":['آ','أ','إ']\n",
    "        ,\"ء\":['ؤ','ئ']      \n",
    "               }\n",
    "    for main,syn in synonyms.items():\n",
    "        for s in syn:\n",
    "            tweet = re.sub(str(s),str(main),tweet)\n",
    "    return tweet       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b0b9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet, flags=[1,1,1,1,1,1,1,1,1,1,1]):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "        flags: list of \"work\" values for all functions will called here.\n",
    "    Output:\n",
    "        cleaned_tweet: tweet after apply all cleaning and normlizaing functions\n",
    "\n",
    "    \"\"\"\n",
    "    tweet = remove_urls(tweet, flags[0])\n",
    "    tweet = remove_mentions(tweet, flags[1])\n",
    "    tweet = remove_repeated(tweet,flags[2])\n",
    "    tweet = unify(tweet,flags[3])\n",
    "    tweet = unify_c(tweet,flags[4])\n",
    "    tweet = remove_numbers(tweet,flags[5])\n",
    "    tweet = remove_emojis(tweet,flags[6])\n",
    "    tweet = remove_sWords(tweet,flags[7])\n",
    "    tweet = stemming(tweet,flags[8])\n",
    "    tweet = remove_nonar(tweet,flags[9])  \n",
    "    tweet = remove_punc(tweet,flags[10])\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def preprocess(arr,text_field,flags = [1,1,1,1,1,1,1,1,1,1,1]):\n",
    "    arr['tweet'] = arr[text_field].apply(lambda x: preprocess_tweet(str(x),flags))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4969b90",
   "metadata": {},
   "source": [
    "## [3.3] Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99144a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "805fa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t,vec = tfidf(pre)\n",
    "Y_train_t = Y_pre\n",
    "X_test_t = vec.transform(X_test)\n",
    "Y_test_t = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33916c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_t, Y_train_t)\n",
    "\n",
    "y_predicted_train_t = clf.predict(X_train_t)\n",
    "y_predicted_test_t = clf.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2323a33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 0.8542564987064186\n",
      "Test : 0.770702981229297\n"
     ]
    }
   ],
   "source": [
    "print(\"Train : \" + str(accuracy_score(Y_train_t, y_predicted_train_t)))\n",
    "accuracy = accuracy_score(Y_test_t, y_predicted_test_t)\n",
    "print(\"Test : \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7d0a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression tfidf\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing URLs, unifying Words, removing Numbers, removing Stop-words\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86e42fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.5, ngram_range=(1, 12))\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abb701",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0e0481e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in train['tweet'].values])\n",
    "print(max_length)\n",
    "input_dim = X_train_n.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "677243a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#الصحة:•نحو 14.5 مليون شخص تم إعطاؤهم جرعة لقاح #كورونا•اللقاحات آمنة وبإمكان كل من تلقى اللقاح التبرع بالدم•يمكن تلقي جرعات مختلفة من لقاح كورونا\n",
      "[16, 15208, 765, 207, 50, 82, 51, 5531, 35, 1, 15209, 435, 15210, 29, 3, 272, 8, 3932, 15211, 98, 79, 1570, 3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "x_train1 = train['tweet'].values\n",
    "x_test1 = test['tweet'].values\n",
    "x_valid1 = valid['tweet'].values\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train1)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(x_train1)\n",
    "Y_train = train['label'].values\n",
    "X_test = tokenizer.texts_to_sequences(x_test1)\n",
    "Y_test = test['label'].values\n",
    "X_valid = tokenizer.texts_to_sequences(x_valid1)\n",
    "Y_valid= valid['label'].values\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "\n",
    "print(x_train1[2])\n",
    "print(X_train[2])\n",
    "\n",
    "maxlen = 213\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9a0be845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 213, 100)          4593400   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 209, 32)           16032     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 104, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3328)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                33290     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,642,733\n",
      "Trainable params: 4,642,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ceb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "757635d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624/1624 - 51s - loss: 0.5191 - accuracy: 0.7275 - val_loss: 0.4329 - val_accuracy: 0.7946 - 51s/epoch - 32ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                     epochs=1,\n",
    "                     verbose=2,\n",
    "                     validation_data=(X_valid, Y_valid),\n",
    "                     batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c893ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7832\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "dabdced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'].append(\"Deep_NN(Embeding,Conv1d,Maxpool)(P/N Classes)\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing URLs, unifying Words, removing Numbers, removing Stop-words\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "6488a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "817cedec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "train = tr[(tr['label'] == 0)|(tr['label'] == 1)|(tr['label'] == 2)|(tr['label'] == 3)]\n",
    "train.dropna(inplace=True)  \n",
    "test = te[(te['label'] == 0)|(te['label'] == 1)|(te['label'] == 2)|(te['label'] == 3)]\n",
    "valid = va[(va['label'] == 0)|(va['label'] == 1)|(va['label'] == 2)|(va['label'] == 3)]\n",
    "x_train1 = train['tweet'].values\n",
    "x_test1 = test['tweet'].values\n",
    "x_valid1 = valid['tweet'].values\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train1)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(x_train1)\n",
    "Y_train = train['label'].values\n",
    "X_test = tokenizer.texts_to_sequences(x_test1)\n",
    "Y_test = test['label'].values\n",
    "X_valid = tokenizer.texts_to_sequences(x_valid1)\n",
    "Y_valid= valid['label'].values\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "y_train = encoder.transform(Y_train)\n",
    "y_test = encoder.transform(Y_test)\n",
    "y_valid = encoder.transform(Y_valid)\n",
    "y_train = np_utils.to_categorical(y_train, 4)\n",
    "y_test = np_utils.to_categorical(y_test, 4)\n",
    "y_valid = np_utils.to_categorical(y_valid, 4)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "\n",
    "max_len = max([len(s.split()) for s in train['tweet'].values])\n",
    "print(max_len)\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "55c9e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 302, 100)          8515100   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 298, 32)           16032     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 149, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4768)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                47690     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,578,866\n",
      "Trainable params: 8,578,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim1 = 150\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=max_len))\n",
    "model1.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(10, activation='relu'))\n",
    "model1.add(layers.Dense(4, activation='softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "acb5806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "692/692 - 44s - loss: 0.4953 - accuracy: 0.4661 - val_loss: 0.4597 - val_accuracy: 0.5269 - 44s/epoch - 64ms/step\n",
      "Epoch 2/3\n",
      "692/692 - 47s - loss: 0.3509 - accuracy: 0.6756 - val_loss: 0.4571 - val_accuracy: 0.5545 - 47s/epoch - 68ms/step\n",
      "Epoch 3/3\n",
      "692/692 - 43s - loss: 0.1616 - accuracy: 0.8864 - val_loss: 0.5581 - val_accuracy: 0.5470 - 43s/epoch - 61ms/step\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, y_train,\n",
    "                     epochs=3,\n",
    "                     verbose=2,\n",
    "                     validation_data=(X_valid, y_valid),\n",
    "                     batch_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "cecbb055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.5670\n"
     ]
    }
   ],
   "source": [
    "loss2, accuracy2 = model1.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "a3d9d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'].append(\"Deep_NN(Embeding,Conv1d,Maxpool)(With all Classes)\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing URLs, unifying Words, removing Numbers, removing Stop-words\")\n",
    "model_comparison_table['accuracy'].append(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "86f8f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ea3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1747f25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>preprocessing_methods</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>none</td>\n",
       "      <td>0.776592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing URLs</td>\n",
       "      <td>0.776960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Mentions</td>\n",
       "      <td>0.771071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Repeated</td>\n",
       "      <td>0.779904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>Unify words</td>\n",
       "      <td>0.774384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>Unify characters</td>\n",
       "      <td>0.770703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Numbers</td>\n",
       "      <td>0.781008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Emojis</td>\n",
       "      <td>0.775488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Stop-words</td>\n",
       "      <td>0.774752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>Stemming</td>\n",
       "      <td>0.770335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Non-arabic</td>\n",
       "      <td>0.773279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing Punctuaion</td>\n",
       "      <td>0.776592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing URLs, unifying Words, removing Number...</td>\n",
       "      <td>0.782481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logistic regression tfidf</td>\n",
       "      <td>removing URLs, unifying Words, removing Number...</td>\n",
       "      <td>0.770703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Deep_NN(Embeding,Conv1d,Maxpool)(P/N Classes)</td>\n",
       "      <td>removing URLs, unifying Words, removing Number...</td>\n",
       "      <td>0.783217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Deep_NN(Embeding,Conv1d,Maxpool)(With all Clas...</td>\n",
       "      <td>removing URLs, unifying Words, removing Number...</td>\n",
       "      <td>0.566956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           model_name  \\\n",
       "0               logistic regression with bag of words   \n",
       "1               logistic regression with bag of words   \n",
       "2               logistic regression with bag of words   \n",
       "3               logistic regression with bag of words   \n",
       "4               logistic regression with bag of words   \n",
       "5               logistic regression with bag of words   \n",
       "6               logistic regression with bag of words   \n",
       "7               logistic regression with bag of words   \n",
       "8               logistic regression with bag of words   \n",
       "9               logistic regression with bag of words   \n",
       "10              logistic regression with bag of words   \n",
       "11              logistic regression with bag of words   \n",
       "12              logistic regression with bag of words   \n",
       "13                          logistic regression tfidf   \n",
       "14      Deep_NN(Embeding,Conv1d,Maxpool)(P/N Classes)   \n",
       "15  Deep_NN(Embeding,Conv1d,Maxpool)(With all Clas...   \n",
       "\n",
       "                                preprocessing_methods  accuracy  \n",
       "0                                                none  0.776592  \n",
       "1                                       removing URLs  0.776960  \n",
       "2                                   removing Mentions  0.771071  \n",
       "3                                   removing Repeated  0.779904  \n",
       "4                                         Unify words  0.774384  \n",
       "5                                    Unify characters  0.770703  \n",
       "6                                    removing Numbers  0.781008  \n",
       "7                                     removing Emojis  0.775488  \n",
       "8                                 removing Stop-words  0.774752  \n",
       "9                                            Stemming  0.770335  \n",
       "10                                removing Non-arabic  0.773279  \n",
       "11                                removing Punctuaion  0.776592  \n",
       "12  removing URLs, unifying Words, removing Number...  0.782481  \n",
       "13  removing URLs, unifying Words, removing Number...  0.770703  \n",
       "14  removing URLs, unifying Words, removing Number...  0.783217  \n",
       "15  removing URLs, unifying Words, removing Number...  0.566956  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(model_comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e307e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"[محمد سعيد جديني].csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59e22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
